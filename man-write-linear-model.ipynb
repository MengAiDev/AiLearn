{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import random\n\nclass LinearModel:\n    def __init__(self) -> None:\n        ### Model: ax + b ###\n        self.weight_a: float = random.uniform(0, 2)\n        self.weight_b: float = random.uniform(0, 2)\n        self.learning_rate: float = 0.008  # 添加学习率\n\n    def predict(self, model_input: float | int) -> float:\n        \"\"\"预测函数\"\"\"\n        return self.weight_a * model_input + self.weight_b\n\n    def loss(self, model_input: float | int, actual_output: float | int) -> float:\n        \"\"\"计算单个样本的损失\"\"\"\n        prediction = self.predict(model_input)\n        return (prediction - actual_output) ** 2\n\n    def calculate_gradients(self, X: list[float | int], y: list[float | int]) -> tuple[float, float]:\n        \"\"\"计算整个数据集的梯度平均值\"\"\"\n        grad_a_total = 0.0\n        grad_b_total = 0.0\n        n = len(X)\n        \n        for i in range(n):\n            prediction = self.predict(X[i])\n            error = prediction - y[i]  # 注意这里是 prediction - y，不是 y - prediction\n            \n            # 计算梯度\n            grad_a_total += error * X[i]  # ∂L/∂a = error * x\n            grad_b_total += error         # ∂L/∂b = error\n        \n        # 返回平均梯度\n        return grad_a_total / n, grad_b_total / n\n\n    def update_weights(self, grad_a: float, grad_b: float) -> None:\n        \"\"\"使用梯度下降更新权重\"\"\"\n        self.weight_a -= self.learning_rate * grad_a\n        self.weight_b -= self.learning_rate * grad_b\n\n    def fit(self, X: list[float | int], y: list[float | int], max_epochs: int = 1000, target_loss: float = 1e-6) -> None:\n        \"\"\"训练模型\"\"\"\n        if len(X) != len(y):\n            raise ValueError(f'Shapes are not same: {len(X)} != {len(y)}')\n        \n        total_loss = float('inf')\n        epoch = 0\n        \n        while epoch < max_epochs and total_loss > target_loss:\n            # 计算梯度\n            grad_a, grad_b = self.calculate_gradients(X, y)\n            \n            # 更新权重\n            self.update_weights(grad_a, grad_b)\n            \n            # 计算当前epoch的总损失\n            total_loss = 0\n            for i in range(len(X)):\n                total_loss += self.loss(X[i], y[i])\n            total_loss /= len(X)\n            \n            # 打印训练信息（每100个epoch打印一次）\n            if epoch % 100 == 0:\n                print(f\"Epoch: {epoch}, Loss: {total_loss:.6f}, a: {self.weight_a:.6f}, b: {self.weight_b:.6f}\")\n            \n            epoch += 1\n        \n        print(f\"Training completed after {epoch} epochs\")\n        print(f\"Final - Loss: {total_loss:.6f}, a: {self.weight_a:.6f}, b: {self.weight_b:.6f}\")\n\n# 使用示例\nif __name__ == \"__main__\":\n    # 创建一些测试数据 (y = 2x + 1)\n    X_train = [1, 2, 3, 4, 5]\n    y_train = [3, 5, 7, 9, 11]  # 2*x + 1\n    \n    # 创建并训练模型\n    model = LinearModel()\n    print(f\"Initial weights: a={model.weight_a:.4f}, b={model.weight_b:.4f}\")\n    \n    model.fit(X_train, y_train, max_epochs=100000)\n    \n    # 测试预测\n    test_x = 6\n    prediction = model.predict(test_x)\n    print(f\"Prediction for x={test_x}: {prediction:.4f} (Expected: {2*test_x+1})\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-09-07T10:39:36.373111Z","iopub.execute_input":"2025-09-07T10:39:36.373457Z","iopub.status.idle":"2025-09-07T10:39:36.398316Z","shell.execute_reply.started":"2025-09-07T10:39:36.373432Z","shell.execute_reply":"2025-09-07T10:39:36.397251Z"}},"outputs":[{"name":"stdout","text":"Initial weights: a=0.3727, b=0.7181\nEpoch: 0, Loss: 26.198662, a: 0.522642, b: 0.759443\nEpoch: 100, Loss: 0.003407, a: 1.962038, b: 1.136786\nEpoch: 200, Loss: 0.002599, a: 1.966903, b: 1.119490\nEpoch: 300, Loss: 0.001983, a: 1.971092, b: 1.104365\nEpoch: 400, Loss: 0.001512, a: 1.974751, b: 1.091156\nEpoch: 500, Loss: 0.001154, a: 1.977947, b: 1.079618\nEpoch: 600, Loss: 0.000880, a: 1.980738, b: 1.069540\nEpoch: 700, Loss: 0.000671, a: 1.983176, b: 1.060739\nEpoch: 800, Loss: 0.000512, a: 1.985306, b: 1.053051\nEpoch: 900, Loss: 0.000391, a: 1.987166, b: 1.046336\nEpoch: 1000, Loss: 0.000298, a: 1.988790, b: 1.040471\nEpoch: 1100, Loss: 0.000227, a: 1.990209, b: 1.035349\nEpoch: 1200, Loss: 0.000174, a: 1.991448, b: 1.030874\nEpoch: 1300, Loss: 0.000132, a: 1.992531, b: 1.026967\nEpoch: 1400, Loss: 0.000101, a: 1.993476, b: 1.023553\nEpoch: 1500, Loss: 0.000077, a: 1.994302, b: 1.020572\nEpoch: 1600, Loss: 0.000059, a: 1.995023, b: 1.017968\nEpoch: 1700, Loss: 0.000045, a: 1.995653, b: 1.015694\nEpoch: 1800, Loss: 0.000034, a: 1.996203, b: 1.013708\nEpoch: 1900, Loss: 0.000026, a: 1.996684, b: 1.011973\nEpoch: 2000, Loss: 0.000020, a: 1.997104, b: 1.010457\nEpoch: 2100, Loss: 0.000015, a: 1.997470, b: 1.009134\nEpoch: 2200, Loss: 0.000012, a: 1.997790, b: 1.007978\nEpoch: 2300, Loss: 0.000009, a: 1.998070, b: 1.006968\nEpoch: 2400, Loss: 0.000007, a: 1.998314, b: 1.006086\nEpoch: 2500, Loss: 0.000005, a: 1.998528, b: 1.005316\nEpoch: 2600, Loss: 0.000004, a: 1.998714, b: 1.004643\nEpoch: 2700, Loss: 0.000003, a: 1.998877, b: 1.004055\nEpoch: 2800, Loss: 0.000002, a: 1.999019, b: 1.003542\nEpoch: 2900, Loss: 0.000002, a: 1.999143, b: 1.003094\nEpoch: 3000, Loss: 0.000001, a: 1.999252, b: 1.002702\nEpoch: 3100, Loss: 0.000001, a: 1.999346, b: 1.002360\nTraining completed after 3107 epochs\nFinal - Loss: 0.000001, a: 1.999352, b: 1.002341\nPrediction for x=6: 12.9985 (Expected: 13)\n","output_type":"stream"}],"execution_count":3}]}